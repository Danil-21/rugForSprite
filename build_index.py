import re
import logging
from pathlib import Path
from typing import List
from bs4 import BeautifulSoup
from langchain_community.document_loaders import (
    DirectoryLoader,
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from rag_config import DATA_RAW_DIR, VECTOR_DB_DIR

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å–ª–æ–≤
    text = re.sub(r"(\w+)-\n(\w+)", r"\1\2", text)
    
    patterns = [
        r"–°—Ç—Ä\.\s*\d+\s+–∏–∑\s+\d+",
        r"\d+\s+—Å—Ç—Ä–∞–Ω–∏—Ü–∞\s+–∏–∑\s+\d+",
        r"¬©.*?(–°–±–µ—Ä–±–∞–Ω–∫|Sberbank|20\d{2}|\d{4})",
        r"–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ|–î–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è|–í–µ—Ä—Å–∏—è\s*\d+",
        r"ID\s*–¥–æ–∫—É–º–µ–Ω—Ç–∞[:\s]*[A-Z0-9\-]+",
        r"–¢–µ–ª\.:?\s*900|–¢–µ–ª–µ—Ñ–æ–Ω:\s*900",
        r"\s{2,}",
    ]
    
    for p in patterns:
        text = re.sub(p, "", text, flags=re.IGNORECASE)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    text = re.sub(r"\n\s*\n+", "\n\n", text)
    return text.strip()


def load_html_documents(path: Path) -> List[Document]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏"""
    docs = []
    
    for file in path.glob("*.html"):
        try:
            with open(file, encoding="utf-8") as f:
                html = f.read()
            soup = BeautifulSoup(html, "html.parser")
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for tag in soup.find_all(['header', 'nav', 'footer', 'aside', 'script', 'style']):
                tag.decompose()
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
            if not main_content:
                main_content = soup.body
            
            if not main_content:
                continue
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            text_parts = []
            for element in main_content.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'td', 'th', 'div']):
                element_text = element.get_text(strip=True, separator=' ')
                if element_text and len(element_text) > 20:  # –£–≤–µ–ª–∏—á–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                    if element.name in ['h1', 'h2', 'h3', 'h4']:
                        prefix = "#" * int(element.name[1]) + " "
                        text_parts.append(prefix + element_text)
                    else:
                        text_parts.append(element_text)
            
            if text_parts:
                full_text = "\n".join(text_parts)
                cleaned_text = clean_text(full_text)
                if cleaned_text:
                    docs.append(Document(
                        page_content=cleaned_text,
                        metadata={"source": str(file), "type": "html"}
                    ))
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω HTML: {file} - {len(cleaned_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file}: {e}")
    
    return docs

def smart_chunking(docs: List[Document]) -> List[Document]:
    """–£–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –∏—Ö —Ç–∏–ø–∞"""
    all_chunks = []
    
    for doc in docs:
        content = doc.page_content
        metadata = doc.metadata.copy()
        
        # –î–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        doc_type = metadata.get("type", "unknown")
        source = metadata.get("source", "")
        
        if doc_type == "html" or source.endswith(".html"):
            # –î–ª—è HTML —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
            sections = []
            current_section = []
            current_title = "–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
            
            lines = content.split('\n')
            for line in lines:
                line_stripped = line.strip()
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                if line_stripped.startswith('#'):
                    if current_section:
                        section_text = '\n'.join(current_section)
                        if len(section_text) > 50:
                            sections.append((current_title, section_text))
                        current_section = []
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    current_title = line_stripped.lstrip('#').strip()
                else:
                    current_section.append(line)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
            if current_section:
                section_text = '\n'.join(current_section)
                if len(section_text) > 50:
                    sections.append((current_title, section_text))
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ —Å–µ–∫—Ü–∏–π
            for title, section_text in sections:
                # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è –±–æ–ª—å—à–∞—è, —Ä–∞–∑–±–∏–≤–∞–µ–º –¥–∞–ª—å—à–µ
                if len(section_text) > 1000:
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=800,
                        chunk_overlap=100,
                        separators=["\n\n", "\n", ". ", "! ", "? "],
                    )
                    sub_chunks = splitter.split_documents([
                        Document(page_content=section_text, metadata=metadata)
                    ])
                    for i, chunk in enumerate(sub_chunks):
                        chunk.metadata.update({
                            **metadata,
                            "section": f"{title} (—á–∞—Å—Ç—å {i+1})"
                        })
                        all_chunks.append(chunk)
                else:
                    chunk = Document(
                        page_content=section_text,
                        metadata={**metadata, "section": title}
                    )
                    all_chunks.append(chunk)
        
        else:
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,  # –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", "! ", "? ", "; ", " ", ""],
                length_function=len,
            )
            chunks = splitter.split_documents([doc])
            for chunk in chunks:
                chunk.metadata.update(metadata)
            all_chunks.extend(chunks)
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    return all_chunks

def load_documents() -> List[Document]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
    all_docs = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
    patterns = {
        "**/*.txt": (TextLoader, {"encoding": "utf-8"}),
        "**/*.pdf": (PyPDFLoader, {}),
        "**/*.docx": (Docx2txtLoader, {}),
        "**/*.doc": (Docx2txtLoader, {}),
    }
    
    for glob_pattern, (loader_cls, loader_kwargs) in patterns.items():
        try:
            loader = DirectoryLoader(
                str(DATA_RAW_DIR),
                glob=glob_pattern,
                loader_cls=loader_cls,
                loader_kwargs=loader_kwargs,
                show_progress=True,
            )
            docs = loader.load()
            for doc in docs:
                doc.page_content = clean_text(doc.page_content)
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
                source = doc.metadata.get("source", "")
                if source.endswith(".pdf"):
                    doc.metadata["type"] = "pdf"
                elif source.endswith((".docx", ".doc")):
                    doc.metadata["type"] = "doc"
                else:
                    doc.metadata["type"] = "text"
            all_docs.extend(docs)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {glob_pattern}")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã {glob_pattern}: {e}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º HTML –¥–æ–∫—É–º–µ–Ω—Ç—ã
    html_path = DATA_RAW_DIR / "web"
    if html_path.exists():
        html_docs = load_html_documents(html_path)
        all_docs.extend(html_docs)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(html_docs)} HTML –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    unique_docs = []
    seen_content = set()
    
    for doc in all_docs:
        if doc.page_content.strip() and len(doc.page_content.strip()) > 50:
            content_hash = hash(doc.page_content[:500])  # –•—ç—à –ø–µ—Ä–≤—ã—Ö 500 —Å–∏–º–≤–æ–ª–æ–≤
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_docs.append(doc)
    
    logger.info(f"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(unique_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    return unique_docs

def build_index():
    """–°—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    DATA_RAW_DIR.mkdir(parents=True, exist_ok=True)
    VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)
    
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    docs = load_documents()
    
    if not docs:
        logger.warning("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        print("‚ö†Ô∏è –í –ø–∞–ø–∫–µ data/raw –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –î–æ–±–∞–≤—å—Ç–µ —Ñ–∞–π–ª—ã –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö: txt, pdf, docx, html")
        return
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –£–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
    logger.info("–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
    chunks = smart_chunking(docs)
    
    if not chunks:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏")
        return
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å
    logger.info("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
    vectordb = FAISS.from_documents(chunks, embeddings)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
    vectordb.save_local(str(VECTOR_DB_DIR))
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
    index_files = list(VECTOR_DB_DIR.glob("*"))
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(index_files)} —Ñ–∞–π–ª–æ–≤ –∏–Ω–¥–µ–∫—Å–∞")
    
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω!")
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
    print(f"   - –ß–∞–Ω–∫–æ–≤: {len(chunks)}")
    print(f"   - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {sum(len(c.page_content) for c in chunks) // len(chunks)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   - –ü—É—Ç—å –∫ –∏–Ω–¥–µ–∫—Å—É: {VECTOR_DB_DIR}")

if __name__ == "__main__":
    build_index()
